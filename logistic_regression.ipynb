{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS6rOSGI4adL0NN0r9jabk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobley-trent/ml-from-scratch/blob/mobley-trent-patch-1/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical method used for binary classification, where the goal is to predict whether a given input belongs to one of two classes.  It is a type of regression analysis that models the relationship between the dependent variable (the binary output) and one or more independent variables (the input features) using a logistic function.\n",
        "\n",
        "The logistic function, also known as the sigmoid function, is an S-shaped curve that maps any real-valued input to a value between 0 and 1. The logistic function is defined as follows:\n",
        "\n",
        "`f(z) = 1 / (1 + e^(-z))`\n",
        "\n",
        "where z is a linear combination of the input features and their associated weights:\n",
        "\n",
        "`z = w0 + w1x1 + w2x2 + ... + wn*xn`\n",
        "\n",
        "where `w0` is the bias term and `x1, x2, ..., xn` are the input features. The weights `w1, w2, ..., wn` represent the strength and direction of the relationship between each input feature and the output."
      ],
      "metadata": {
        "id": "37rjjEbz7D0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro-iq4Pb6aZP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class LogisticRegression():\n",
        "\n",
        "    def __init__(self, lr=0.001, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_pred = np.dot(X, self.weights) + self.bias\n",
        "            predictions = sigmoid(linear_pred)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1/n_samples) * np.sum(predictions-y)\n",
        "\n",
        "            self.weights = self.weights - self.lr*dw\n",
        "            self.bias = self.bias - self.lr*db\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_pred = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = sigmoid(linear_pred)\n",
        "        class_pred = [0 if y <= 0.5 else 1 for y in y_pred]\n",
        "        return class_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression algorithm works by minimizing a cost function using an optimization technique such as gradient descent. The cost function measures the difference between the predicted output and the true output for each training example. The goal is to find the set of weights that minimizes the cost function and produces the most accurate predictions.\n",
        "\n",
        "Once the weights are determined, the logistic regression model can be used to predict the output for new input values by computing the logistic function for the corresponding value of z. If the output is greater than a threshold (usually 0.5), the input is classified as belonging to the positive class; otherwise, it is classified as belonging to the negative class."
      ],
      "metadata": {
        "id": "4orwIs9w9srl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "lUlQxwec9y73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "    return np.sum(y_pred==y_test)/len(y_test)"
      ],
      "metadata": {
        "id": "TQjOx6DA-F0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "acc = accuracy(y_pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6vobeNq9_Yc",
        "outputId": "3ade2248-9603-4f6b-ecc6-ec815b8f5161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8947368421052632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3837ce32b293>:4: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try increasing the learning rate to see if this will have an effect on the accuracy of the model."
      ],
      "metadata": {
        "id": "0av8lNb3-TB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(lr=0.01)\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "acc = accuracy(y_pred, y_test)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdrdkRTx-LBc",
        "outputId": "4701e0ab-19c8-49b7-ddcf-3272f3797147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9210526315789473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3837ce32b293>:4: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a popular algorithm used in a variety of applications, such as fraud detection, credit scoring, customer segmentation, and medical diagnosis. It is a simple and interpretable model that can handle both categorical and continuous input variables. However, it assumes a linear relationship between the input features and the output, which may not always be the case in real-world scenarios. Additionally, it is sensitive to outliers and multicollinearity, which can affect the accuracy of the model."
      ],
      "metadata": {
        "id": "7C-XVo8W-mPs"
      }
    }
  ]
}