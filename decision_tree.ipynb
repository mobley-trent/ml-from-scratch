{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQpLl9VdMyDtfR/uarhv13",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobley-trent/ml-from-scratch/blob/mobley-trent-patch-1/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a supervised learning algorithm that is used for classification and regression modeling. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. It is one of the most widely used and practical methods for supervised learning.\n",
        "\n",
        "Decision trees are constructed via an algorithmic approach that identifies ways to split a data set based on various conditions. There are two types of decision trees: classification trees and regression trees. Classification trees are used when the target variable is categorical, while regression trees are used when the target variable is continuous."
      ],
      "metadata": {
        "id": "Kj2drAOc-v3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyW7Rl2AjUFU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Node` class represents a node in the decision tree, which has a feature and threshold to split the data, as well as left and right child nodes. It also has a value that is assigned when the node is a leaf node. The `is_leaf_node()` method checks whether the node is a leaf node or not.\n",
        "\n",
        "The DecisionTree class has the following attributes:\n",
        "\n",
        "- `min_samples_split`: the minimum number of samples required to split an internal node.\n",
        "- `max_depth`: the maximum depth of the tree.\n",
        "- `n_features`: the number of features to consider when looking for the best split. If `None`, then all features are considered.\n",
        "- `root`: the root node of the decision tree."
      ],
      "metadata": {
        "id": "1OxUF2iLKp81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `fit()` method takes the input data X and corresponding labels y and grows the decision tree using the `_grow_tree()` method.\n",
        "\n",
        "The `_grow_tree()` method recursively grows the decision tree. It checks if the stopping criteria is met or not. If met, it assigns a leaf value to the node. Otherwise, it randomly selects a subset of features, finds the best split using the `_best_split()` method, creates child nodes using the `_split()` method, and grows the left and right sub-trees recursively.\n",
        "\n",
        "The `_best_split()` method finds the feature and threshold that results in the highest information gain using the `_information_gain()` method. It iterates through all features and their unique thresholds to find the best split.\n",
        "\n",
        "The `_information_gain()` method calculates the information gain of a split by calculating the **entropy** of the parent and child nodes. It calculates the entropy of the left and right child nodes using the `_entropy()` method and returns the information gain.\n",
        "\n",
        "The `_split()` method splits the data into left and right indices based on the feature threshold.\n",
        "\n",
        "The `_entropy()` method calculates the entropy of the input labels y.\n",
        "\n",
        "The `_most_common_label()` method returns the most common label in the input labels y.\n",
        "\n",
        "The `predict()` method takes input data X and returns the predicted labels by traversing the decision tree using the `_traverse_tree()` method.\n",
        "\n",
        "The `_traverse_tree()` method traverses the decision tree recursively by checking the node feature and threshold and moving down to the left or right child node until it reaches a leaf node, which returns the assigned value."
      ],
      "metadata": {
        "id": "aUoN3WabLUg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the algorithm with data from sklearn.datasets\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1234\n",
        ")"
      ],
      "metadata": {
        "id": "Cr-8fNDyLT8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTree(max_depth=10)\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "TAbfX-fiNLIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_test, y_pred):\n",
        "    return np.sum(y_test == y_pred) / len(y_test)\n",
        "\n",
        "acc = accuracy(y_test, predictions)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e304Ee8NQiT",
        "outputId": "380511a9-223d-4514-b0a6-5e087c04f597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9210526315789473\n"
          ]
        }
      ]
    }
  ]
}