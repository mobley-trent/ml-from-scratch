{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1ObiW9BfPlVQi9fV+lsqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobley-trent/ml-from-scratch/blob/master/random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is a popular machine learning algorithm used for both classification and regression tasks. It is an ensemble learning method that combines multiple decision trees to improve the accuracy and stability of predictions.\n",
        "\n",
        "In Random Forest, a set of decision trees are built using a random subset of the available data and a random subset of the available features. The data subsets are sampled with replacement using a technique called *bootstrapping*, which creates multiple versions of the original dataset. The feature subsets are selected randomly, with each tree being trained on a different subset of features.\n",
        "\n",
        "Once the decision trees are built, the algorithm uses them to make predictions on new data by aggregating the predictions of all the trees. In the case of classification, the final prediction is typically the mode of the predicted classes from all the trees, while for regression, the final prediction is usually the mean of the predicted values from all the trees.\n",
        "\n",
        "Random Forest has several advantages over individual decision trees. First, by averaging the predictions of multiple trees, it reduces the tendency of decision trees to overfit the training data. Second, by randomly selecting subsets of data and features, it creates a diverse set of decision trees that are less correlated with each other, leading to better generalization performance. Finally, Random Forest is relatively easy to use and can handle large datasets with high-dimensional feature spaces.\n",
        "\n",
        "Overall, Random Forest is a powerful and widely-used machine learning algorithm that can produce accurate and robust predictions for a variety of tasks."
      ],
      "metadata": {
        "id": "xFcGYJmq3Qfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppNZxK06wFLc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_feature=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth=max_depth\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.n_features=n_feature\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(max_depth=self.max_depth,\n",
        "                            min_samples_split=self.min_samples_split,\n",
        "                            n_features=self.n_features)\n",
        "            X_sample, y_sample = self._bootstrap_samples(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def _bootstrap_samples(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[idxs], y[idxs]\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(predictions, 0, 1)\n",
        "        predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "Mg4LBTX11gQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import random\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1234\n",
        ")\n"
      ],
      "metadata": {
        "id": "ISEcIb3z12DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "kBJQNIf_2UT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = RandomForest(n_trees=20)\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "acc =  accuracy(y_test, predictions)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om_XPz782dtL",
        "outputId": "d8f1b03a-baf4-404d-eba3-93a6bda15a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9122807017543859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest algorithm can be used for a variety of classification and regression tasks, including but not limited to:\n",
        "\n",
        "- Image classification: Random Forest can be used for image classification tasks, such as recognizing different objects or identifying patterns in images.\n",
        "- Fraud detection: Random Forest can be used for fraud detection tasks by identifying patterns in transaction data that may indicate fraudulent activity.\n",
        "- Customer segmentation: Random Forest can be used for customer segmentation tasks by grouping customers based on their purchasing behavior, demographic information, or other characteristics.\n",
        "- Medical diagnosis: Random Forest can be used for medical diagnosis tasks, such as identifying diseases based on patient symptoms, medical history, and test results.\n",
        "- Natural language processing: Random Forest can be used for natural language processing tasks, such as sentiment analysis, text classification, and spam filtering."
      ],
      "metadata": {
        "id": "NL4VaVtj4xqA"
      }
    }
  ]
}